---
title: "Performance Metrics Tables for EMA study"
author: "John Curtin"
editor_options: 
  chunk_output_type: console
---

## Overview



## Set up environment
```{r}
# handle conflicts
options(conflicts.policy = "depends.ok")

library(kableExtra, exclude = "group_rows", quietly = TRUE)
library(tidyverse, quietly = TRUE)

# Paths
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")
path_models <- format_path("studydata/risk/models/ema")
path_data_shared <- format_path("studydata/risk/data_processed/shared")
path_data_ema <- format_path("studydata/risk/data_processed/ema")

# Table format
# options(knitr.kable.NA = '')
```

## Make metrics df
```{r}
metrics_week <- read_csv(file.path(path_models, 
                                   "test_metrics_1week_0_v5_nested.csv"),
                         col_types = cols())
metrics_day <- read_csv(file.path(path_models, 
                                  "test_metrics_1day_0_v5_nested.csv"),
                        col_types = cols())
metrics_hour <- read_csv(file.path(path_models, 
                                   "test_metrics_1hour_0_v5_nested.csv"),
                         col_types = cols())

```


```{r}
metrics <- metrics_week |> 
  mutate(model = "Week") |> 
  bind_rows(metrics_day |> 
              mutate(model = "Day")) |> 
  bind_rows(metrics_hour |> 
              mutate(model = "Hour")) |> 
  group_by(.metric, model) |> 
  summarize(median = median(.estimate), .groups = "drop") |> 
  pivot_wider(names_from = model, values_from = median) |> 
  select(.metric, Week, Day, Hour)

metrics <- metrics[c(4,5,6, 1, 3, 2),]

metrics |> write_csv(here::here("notebooks/analysis_objects", "ema_metrics.csv"))
```

```{r}
# footnote_table_metrics <- "Areas under the receiver operating characteristic curves (auROCs) summarize the model's sensitivity and specificity over all possible decision thresholds. Sensitivity, specificity, balanced accuracy, positive predictive value, and negative predictive value are performance metrics calculated at a single decision threshold for each model determined with Youdenâ€™s index. All metrics represent median values across 30 held-out test sets." 
# 
# metrics |> 
#  mutate(.metric = case_when(.metric == "roc_auc" ~ "auROC",
#                             .metric == "sens" ~ "sensitivity",
#                             .metric == "spec" ~ "specificity",
#                             .metric == "bal_accuracy" ~ "balanced accuracy",
#                             .metric == "ppv" ~ "positive predictive value",
#                             .metric == "npv" ~ "negative predictive value")) |> 
#  kbl(col.names = c("Metric", "Week", "Day", "Hour"),
#      booktabs = TRUE,
#      digits = 2,
#      align = c("l", "l", "l", "l"),
#      linesep = "",
#      caption = "Performance Metrics for Full models by Prediction Window") |>  
#   kable_styling(position = "left", latex_options = c("HOLD_position")) |>  
#   column_spec(column = 1, width = "25em") |>   
#   kableExtra::footnote(general = c(footnote_table_metrics), threeparttable = TRUE)
```


```{r}
metrics |>
  slice(1:4) |>
  mutate(Day = "", Hour = "") |>
  mutate(.metric = case_when(.metric == "roc_auc" ~ "auROC",
                        .metric == "sens" ~ "sensitivity",
                        .metric == "spec" ~ "specificity",
                        .metric == "bal_accuracy" ~ "balanced accuracy")) |>
  kbl(col.names = c("", "Week", "Day", "Hour"),
    digits = 2,
    align = c("r", "c", "c", "c"),
    linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |>
  column_spec(2, color  = "red", bold = TRUE)
```