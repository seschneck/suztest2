---
title: "Smart Digital Therapeutics for Alcohol Use Disorder: Algorithms for Prediction and Adaptive Intervention" 
author: "John J. Curtin, Ph.D."
institute: "University of Wisconsin-Madison"
title-slide-attributes:
  data-background-image: https://github.com/jjcurtin/lectures_science/blob/main/images/smartphone_know_you.png?raw=true
  data-background-size: 35%
  data-background-repeat: no
  data-background-position: left 10% bottom 10%
format: 
  revealjs:
    freeze: auto
    scrollable: true
    chalkboard: true
    css: slides.css
fig-cap-location: top
editor_options: 
  chunk_output_type: console
---
## Mental Healthcare Needs are High and Unmet 

- In 2019, [52 million]{.red} Americans had an active mental illness
  - More than half did not receive any [treatment]{.red}

::: {.notes}
We have a mental health crisis in the U.S. and it is a crisis of **unmet** high need because our delivery of mental healthcare is deeply flawed. 

In 2019, **more than half of the 52 million Americans** with an active mental illness did not receive any treatment. **More than half**!
:::



## Mental Healthcare Needs are High and Unmet  {.smaller}

- [In 2019, 52 million Americans had an active mental illness]{.gray}
  - [More than half did not receive any treatment]{.gray}


- [20 million]{.red} adults had an active substance use disorder
  - 9 out of 10 did not receive any treatment


::: {.notes}


And for the 20 million adults suffering with a substance use disorder, it was worse still.

**9 out of 10 without any treatment**
:::



## Mental Healthcare Needs are High and Unmet  {.smaller}

- [In 2019, 52 million Americans had an active mental illness]{.gray}
  - [More than half did not receive any treatment]{.gray}

- [20 million adults had an active substance use disorder]{.gray}
  - [9 out of 10 did not receive any treatment]{.gray}

- Large treatment disparities exist by race, ethnicity, geography, and income



::: {.notes}
Our failure to treat is even more troubling for vulnerable groups.  Black and LatinX adults receive mental healthcare services at only half the rate of whites.  

And similar mental healthcare disparities exist for people living in rural communities and for those with lower incomes.
:::

## Mental Healthcare Needs are High and Unmet  {.smaller}

- Failure to treat is not surprising given many treatment barriers:
  - Access
  - Availability
  - Affordability
  - Acceptability


::: {.notes}
Our failure to treat is, unfortunately, not surprising.  There are many well known barriers to receiving traditional mental healthcare.

These include problems with access that are particularly limiting for people living in rural communities

Problems with availability

Treatment costs are often prohibitive for those without health insurance 

and stigma and related issues make traditional treatments for mental illness less acceptable to some patients.  
:::

## Digital Therapeutics (DTx)

Digital therapeutics are smartphone "apps" that are designed to prevent, manage, or treat disease, including mental illness. 


Can augment mental health services to address barriers

- Accessible everywhere
- Available 24/7
- Highly scalable (affordable?)

::: {.notes}
Fortunately, digital therapeutics are now being developed and used by patients to address many of these treatment barriers.

For those of you that are not yet familiar with this treatment modality, digital therapeutics are smartphone "apps" that are designed to prevent, manage, or treat disease, including substance use disorders and other mental illness. 

These digital therapeutics can be combined with traditional treatments to reduce barriers because they are

- Accessible everywhere

- Available everyday, 24/7

- and highlighly scalable, which may lower costs
:::

## Digital Therapeutics (DTx)

Digital therapeutics are smartphone "apps" that are designed to prevent, manage, or treat disease, including mental illness. 


Can augment mental health services to address barriers

- [Accessible everywhere]{.gray} 
- [Available 24/7]{.gray} 
- [Highly scalable (affordable?)]{.gray}
- Effective!

::: {.notes}
Of course, these benefits would be meaningless if digital therapeutics were not effective.

But they are.  

For example, patients with substance use disorders who use a digital therapeutic have almost double the odds of being abstinent from alcohol or other drugs

These increases in abstinence from using digital therapeutics are observed not only when compared to patients on wait lists, who have yet to gain access to treatment but also when digital therapeutics are added on top of traditional treatments for substance use disorders.

And these benefits are durable - they have been documented up to 12 months after the start of treatment.  
:::
## {#beta_app data-menu-title="Image of beta app" background-image="https://dionysus.psych.wisc.edu/present/meet_faculty/images/beta_app.png" background-size="100%" background-repeat="none"}


::: {.notes}
These apps are in use today with patients with SUD and at least two have recently received FDA approval.  However, I think they are still best considered beta versions relative to their full potential. 

Their power comes from easy, 24/7 access to their many supports - the treatments, tools, and services built into these smartphone apps. But this is also their Achilles heel. As the patient using these apps, you now have to tackle difficult questions like:

- When should I use them?

- For how long?

- Which of their many supports are best for me?

- And which are best for me **right now**, at this moment in time?
:::

---

## Smart Digital Therapeutics

> “Could you predict not only [who]{.red} might be at greatest risk for relapse … <br>
 … but precisely [when]{.red} that relapse might occur … <br>
 … and how [best to intervene]{.red} to prevent it?"

::: {.notes}
My research team became interested in these issues when my colleague Dave Gustafson, the developer of a leading digital therapeutic for substance use disorders, approached us with a simple question. He asked...

"Could you predict not only who might be at greatest risk for relapse
but precisely when that relapse might occur and
how best to intervene to prevent it"

Dave had just completed a large study demonstrating the effectiveness of his app. However, he also noticed many of the people who relapsed hadn't used the app in the days leading up to that relapse.  And others who had relapsed hadn't used the specific supports in the app that he would have thought would be most effective for them.

The next wave of digital therapeutics, lets call them smart digital therapeutics, must learn to know us well enough to recognize when we are at greatest risk for relapse and they must be smart enough to recommend the specific supports that would be most effective for us at that moment in time to prevent that relapse.

And these apps will do this through the use of built-in artificial intelligence or machine learning algorithms that are powered by personal sensing.

:::

## Lapse Prediction in Patients with AUD
:::: {.columns}

::: {.column width="60%"}
- 151 patients with AUD
- Early in recovery (1-8 weeks)
- Committed to abstinence throughout study
- Followed for up to 3 months
- Collected active and passive personal sensing data streams
:::


::: {.column width="40%"}
![risk1_pis.png](https://github.com/jjcurtin/lectures/blob/main/images/risk1_pis.png?raw=true)\    
![niaaa_logo.png](https://github.com/jjcurtin/lectures/blob/main/images/niaaa_logo.png?raw=true)\   


:::


::::
[GOAL:]{.red} Develop a temporally precise lapse monitoring (prediction) system for patients with AUD




::: {.notes}
So let me transition now to describing how we are taking the first baby steps toward developing smart digital therapeutics for SUD

We have recently completed a NIAAA funded project where we collected data from 151 participants who were in early recovery from a moderate to severe alcohol use disorder.  

These participants were committed to abstinence at the start of the study and we followed them for up to 3 months, collecting a variety of active and passive personal sensing data streams.

Our first goal with this grant was to develop machine learning algorithms that can generate temporally precise predictions about when future lapses back to alcohol use will occur for patients with AUD.

:::

## Personal Sensing Data Streams

- [4X daily ecological momentary assessments (EMA)]{.red}

- Monthly self-report

- Geolocation (GPS)

- Cellular communications (voice and text messages)
  - Meta data
  - Text message content

- Sleep sensor (Wake/sleep times; sleep efficiency; wakings; restlessness)

::: {.notes}
As I mentioned, in this project, we collected a variety of active and passive personal sensing data streams.   

Participants completed brief (7-10 item) ecological momentary assessments or EMAs, 4 times per day

We also have 

- more temporally coarse, monthly self reports, 

- Moment by moment geolocation,

- Meta data from their cellular communications and the actual content of their text messages, 

- and we had sleep sensors in their beds.

We are in the early stages of model building at this point and I will focus today on results from preliminary models using only EMA.  However, we are actively working with GPS and cellular communications as well and I’ll end with some brief discussion of those preliminary models, as well as early plans on how to implement these models clinically to help patients.
:::

## 4x Daily Ecological Momentary Assessments {.medium}


:::: {.columns}

::: {.column width="60%"}
![risk1_ema_questions.png](https://github.com/jjcurtin/lectures/blob/main/images/risk1_ema_questions.png?raw=true){.absolute width="60%" top="15%" height="auto"}\    

:::
::: {.column width="30%"}

- Current
  - Craving
  - Affect
  - Risky situations
  - Stressful events
  - Pleasant events

- Future
  - Risky situations
  - Stressful events
  - Confidence
:::
::::

::: {.notes}
So let me tell you a bit more about the 4x daily EMA we collected.  

On each EMA, participants reported the date and time of any lapses back to alcohol use that they hadn’t previously reported. 

All of the EMAs also asked them about their current craving, affective valence and arousal, recent risky situations, and recent stressful and pleasant events since their last EMA.

On the first EMA each day, they also reported any future risky situations and stressful events that they expected in the next week and their confidence that they would remain abstinent. 
:::

## Feature Engineering


- Features based on recent past experiences (12, 24, 48, 72, 168 hours)

- Min, max, and median response (all items)

- History (count) of past lapses (item 1) and completed EMAs (compliance)

- Raw scores and change scores (from baseline/all past responses)

::: {.notes}
We used these raw EMAs to engineer about 300 features to use in machine learning models to predict future lapses

We formed features by aggregating EMA items over various past time periods ranging from 12 -168 hours in the past

We calculated mins, maxes and medians for the EMA items in these time periods

We also calculated counts of past lapses and counts of past EMAs completed to index engagment with our monitoring system.

And we included these scores both in raw form and as change from baselines for the participant based on all their previous responses since the start of the study.
:::

## Machine Learning Methods


- Predict hour-by-hour probability of [future lapse]{.red}

- Lapse window widths
  - 1 hour
  - 1 day
  - 1 week

::: {.notes}

For our purposes today I wont dive deep into the machine learning methods but let me highlight a few high level details 

We used these features I just described to make predictions about the hour-by-hour probability of a future lapse.  We are developing separate models for three future lapse windows – lapses in the next hour, lapses in the next day, and lapses in the next week.  

For example, if I was in recovery from an AUD, I could use these models to generate the probability that I would lapse after this lecture starting at 1pm.  One model would generate the probability of a lapse between 1pm and 2pm today, the second would predict the probability of a lapse between 1pm today and 1pm tomorrow and the third would provide the probability of a lapse anytime between 1pm today and 1pm next Friday  

And of course, all of the models would only use data collected prior to 1pm today so that they are “predicting”, in the full sense of the word, into the future and not just demonstrating an association.

:::

## Machine Learning Methods {.medium}

- Statistical Algorithms
  - ElasticNet GLM (e.g., LASSO, ridge regression)
  - Random Forest
  - XGBoost
  - KNN

- Model Tuning and Performance Evaluation
  - Area under ROC curve (AUC) as primary performance metric
  - Sensitivity, Specificity, Balanced accuracy, Positive predictive value
  - Using [grouped]{.red} (by participant) [10-fold CV]{.red} 

::: {.notes}
We are evaluating machine learning model configurations that differ by common statistical algorithms. 

We are evaluating these models primarily using the area under the ROC curve but we also consider and report a variety of other common metrics.

And, of course, these performance metrics are calculated for new observations and new participants that the models have never seen and were not trained on by using grouped 10-fold cross-validation.

:::

## 1 Week: Probabilities for No Lapse and Lapse

:::: {.columns}

::: {.column width="60%"}

- Model predicts [probability]{.red} of lapse in next week for “[new]{.red}” observations in test set

- Can panel predictions by [Ground Truth]{.red} (i.e., true lapse vs. no lapse observations

- Want high probabilities to be high for true lapses and low for true no lapses
:::


::: {.column width="40%"}
<!--Lapse Probability by Ground Truth"-->
{{< embed ../../notebooks/ema_figs_probability.qmd#fig-week-no_dec_thres >}}


:::
::::

::: {.notes}
Ok, lets start with the model that provides the coarsest level of temporal specificity – 1 week, and let me take a moment to make the predictions that this machine learning model provides more concrete for you

On the right, you are looking at histograms of the lapse probability predictions that the model makes for all the weeks for all the patients in the held out folds.   

I’ve paneled these histograms by whether a lapse did or did not happen in reality for each predicted week.  The top panel is for weeks with lapses and the bottom panel is for weeks with no lapses.

Ideally, you want the predicted probabilities to be very high for weeks when there was a lapse and very low for weeks when there was no lapse.    

And this is exactly what we see for the one week lapse window model
:::


## 1 Week: Probabilities for No Lapse and Lapse

:::: {.columns}

::: {.column width="60%"}

- [ Model predicts probability of lapse in next week for “new” observations in test set]{.gray}

- [Can panel predictions for GROUND TRUTH lapse and no lapse observations]{.gray}

- [Want high probabilities to be high for true lapses and low for true no lapses]{.gray}

- Need decision threshold for classification [P(Y = Lapse | X) > 0.50 or Youden's J]

:::


::: {.column width="40%"}
<!--Lapse Probability by Ground Truth-->
{{< embed ../../notebooks/ema_figs_probability.qmd#fig-week >}}

:::

::::
::: {.notes}
Now to move from probabilities to actual categorical decisions – in other words, predicting a lapse or no lapse in some specific week, we need a decision threshold.   A probability of .50 is often used for this threshold, but as we will discuss later, there are times when we might want to choose other thresholds.   

But for now, I will use .5 such that the model will predict “lapse” for all weeks with probabilities > .5 and it will predict “no-lapse” for all weeks with probabilities < .5


:::

## Performance Metrics by Lapse Window Width

<!--TEMP CREATE TABLE IN SLIDE DECK-->
```{r}
#| tbl-cap: Performance Metrics by Model 

library(tidyverse, quietly = TRUE)
library(kableExtra, exclude = "group_rows", quietly = TRUE)

metrics <- read_csv(here::here("notebooks/analysis_objects/ema_metrics.csv"),
         col_types = cols()) |> 
  mutate(.metric = case_when(.metric == "roc_auc" ~ "auROC",
                        .metric == "sens" ~ "sensitivity",
                        .metric == "spec" ~ "specificity",
                        .metric == "bal_accuracy" ~ "balanced accuracy",
                        .metric == "ppv" ~ "ppv"))

metrics |> 
  slice(1:4) |>
  mutate(Day = "", Hour = "") |> 
  kbl(col.names = c("", "Week", "Day", "Hour"),
    digits = 2,
    align = c("r", "c", "c", "c"),
    linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |>
  column_spec(2, color  = "red", bold = TRUE)
```

::: {.notes}
Using this decision threshold, we can now calculate the model’s sensitivity, specificity and balanced accuracy which is just the average of these two.

This one week lapse prediction model correctly predicts “lapse” for 79 percent of the weeks that contain a lapse and it correctly predicts “no-lapse” for 85 percent of the weeks that do not include a lapse.   




:::

## 1 Week: ROC Curve

:::: {.columns}

::: {.column width="50%"}

Area under the ROC curve (AUC)

 - Across all decision thresholds

 - ~.5 (random) – 1.0 (perfect)



::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::

:::


::: {.column width="50%"}

<!--ROC Curve for Week-->
{{< embed ../../notebooks/ema_figs_roc.qmd#fig-roc-week >}}

:::

::::

::: {.notes}
But as I said, depending on the application, we may not want to always use a .5 decision threshold.  And this is where the ROC curve and the area under this curve come into play as a performance metric.   The ROC curve is a plot of the model’s sensitivity by its specificity across all possible decision thresholds.

The area under this curve can range from approximately .5 for a random classifier to 1.0 for a classifier that performs perfectly.  

And the AUC for our 1 week model is .90, which is generally considered excellent performance.  

:::

## 1 Day: ROC Curve


::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::


::: {.notes}
So we were very encouraged by the performance of this model because it exceeded the performance of the only other published week level lapse prediction model that we were aware of.   

But we were also eager to see how well we could do if we required a higher level of temporal precision by developing a day level model.



:::


## 1 Day: ROC Curve

:::: {.columns}

::: {.column width="50%"}

::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::


:::


::: {.column width="50%"}

<!--ROC Curve for Week & Day-->
{{< embed ../../notebooks/ema_figs_roc.qmd#fig-roc-week_day >}}

:::
::::

::: {.notes}
And here is the ROC curve and the AUC for the day level model in green, superimposed on the week level model in orange

The day level model performed as well as, if not better, than the week level model with an AUC of .91

[PAUSE]





:::

## Performance Metrics by Lapse Window Width

<!--TEMP CREATE TABLE IN SLIDE DECK-->
```{r}
#| tbl-cap: Performance Metrics by Model 

metrics |> 
  slice(1:4) |>
  mutate(Hour = "") |>
  kbl(col.names = c("", "Week", "Day", "Hour"),
    digits = 2,
    align = c("r", "c", "c", "c"),
    linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |>
  column_spec(3, color  = "red", bold = TRUE)
```

::: {.notes}
And consistent with this higher AUC, the day level model also had slightly better sensitivity and balanced accuracy, along with comparable specificity to the week level model

[PAUSE]


:::


## 1 Hour: ROC Curve


::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::


::: {.notes}
Given our success with this day level model, we next developed a model with the highest level of temporal precision we could build given how we measured lapses, which was the hour level model.  

:::




## 1 Hour: ROC Curve

:::: {.columns}

::: {.column width="50%"}

::: {.absolute left="0%" bottom="0%" width="300px" height="auto"}
::: {style="font-size: 75%;"}
:::{.callout-tip icon=false}
## Coarse rules of thumb for AUC 
.70 - .80 are considered fair  
.80 - .90 are considered good  
\> .90 are considered excellent  
:::
:::
:::


:::


::: {.column width="50%"}

<!--ROC Curve for All-->
{{< embed ../../notebooks/ema_figs_roc.qmd#fig-roc-all >}}

:::
::::

::: {.notes}

And it turns out that we can do somewhat better still with hour level predictions.  Here the blur curve represents the ROC curve for the hour level model, which had the best AUC yet, .93

[PAUSE]


:::


## Performance Metrics by Lapse Window Width

<!--TEMP CREATE TABLE IN SLIDE DECK-->
```{r}
#| tbl-cap: Performance Metrics by Model 

metrics |> 
  slice(1:4) |>
  kbl(col.names = c("", "Week", "Day", "Hour"),
    digits = 2,
    align = c("r", "c", "c", "c"),
    linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |>
  column_spec(4, color  = "red", bold = TRUE)
```

::: {.notes}
And again, consistent with that AUC, the sensitivity, specificity, and balanced accuracy were higher still for this model.

[PAUSE]

:::

## Global Feature Importance by Model{.medium}

:::: {.columns}

::: {.column width="50%"}

- All EMA items impact lapse probability (both globally and locally)

:::

::: {.column width="50%"}

<!--Global SHAP Plot-->
{{< embed ../../notebooks/ema_figs_shaps.qmd#fig-global >}}

:::

::::

::: {.notes}
In the spirit of making this model more transparent and interpretable, lets briefly look under the hood at the feature importance

The plot on the right shows feature names and their associated importance indexed by mean absolute SHAP values over all observations.  From this we see a few important characteristics of the model.   

First, all of EMA items affect predictions about lapse probability globally across observations  As you might expect, history of past lapses has a big influence on the probability of a future lapse.  But self reported abstinence efficacy, craving, history of stress events and other features from the EMA all make meaningful contributions to lapse probability across observations.  

To make these values somewhat meaningful for you, let me remind you these Sh
[logodds of .4 = 1.5 odds;  logodds of .7 = double the odds]

:::

## Local Importance

<!-- Local SHAP - Hour model-->
{{< embed ../../notebooks/ema_figs_shaps.qmd#fig-local_hour >}}

<!-- Local SHAP - Day model-->
{{< embed ../../notebooks/ema_figs_shaps.qmd#fig-local_day >}}

<!-- Local SHAP - Week model-->
{{< embed ../../notebooks/ema_figs_shaps.qmd#fig-local_week >}}

::: {.notes}

:::

## Global Variable Importance by Model{.medium}

:::: {.columns}

::: {.column width="50%"}

- [All EMA items impact lapse probability (both globally and locally)]{.gray}
- Demographics not particularly important (but limited race/ethnicity diversity)
- Lapse day and Lapse hour are useful for day and hour level models as expected
:::

::: {.column width="50%"}

<!--Global SHAP Plot-->
{{< embed ../../notebooks/ema_figs_shaps.qmd#fig-global >}}

:::

::::
::: {.notes}
Returning back to the global feature importance plot, we also see that demographics were not particularly important for prediction lapses for these models.   However, I'll return later to acknowledged that we had limited race/ethnicity diversity in the sample.  Nonetheless, we had reasonable variability on other participant characteristics like sex, SES, education, and addiction severity.  

And finally, we see that we can use lapse day and lapse hour to make predictions with the more temporally precise models.  Not surprisingly, people are more likely to lapse on weekends and during evening hours and the associated day and hour level models models can use that information to improve their lapse predictions.  These features likely contribute to the superior performance of the hour and to a lesser extent the day model relative to the week interval prediction model.

:::


## Positive Predictive Value (PPV)

:::: {.columns}

::: {.column width="50%"}

```{r}
#| tbl-cap: Lapse Rates by Model 
 
tibble(row_name = "Lapse Rate", Week = "25.4%", Day = "7.7%", Hour = "0.4%") |> 
  kbl(format = "html", col.names = c("", "Week", "Day", "Hour"),
      digits = 2,
      align = c("r", "c", "c", "c"),
     linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |> 
  row_spec(1, color  = "red", bold = TRUE)
```


:::

::: {.column width="50%"}


:::
  
::::
  
::: {.notes}
Now, lets return one more time to the performance of these models because I would be remiss if I didn’t complicate the story a bit before we close.  

Clearly, the sensitivity, specificity, and balanced accuracy of all three of these models is encouragingly high

But what is often missed when we evaluate the performance of machine learning models is their positive predictive value or PPV. 
PPV looks at the percentage of positive predictions from the model that are actually true lapses. And PPV is often lower when the positive event, in our case, lapses, is infrequent.

We haven’t talked about the frequency of lapses across the three lapse windows, but it should be intuitively clear that the percent of weeks that contain a lapse will be higher than the percent of days that contain a lapse, and that the percent of hours that contain a lapse will be lower still.  

And this is what we see in our data.

But of course, as we increase the decision threshold for labeling a window as a lapse, we will trade off sensitivity.   We can see this trade off directly in the precision-recall curves on the right.  If we decide we need PPV of at least .75, you can see that we still have reasonable sensitivity for the one week window but we start to miss many lapses in the 1day window and more still in the 1hour window.

I’ll return to this a bit more later when we discuss emerging plans for how best to implement these models within a digital therapeutic.

:::

## Positive Predictive Value (PPV)

:::: {.columns}

::: {.column width="50%"}

```{r}
#| tbl-cap: Lapse Rates by Model 
 
tibble(row_name = "Lapse Rate", Week = "25.4%", Day = "7.7%", Hour = "0.4%") |> 
  kbl(format = "html", col.names = c("", "Week", "Day", "Hour"),
      digits = 2,
      align = c("r", "c", "c", "c"),
     linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |> 
  row_spec(1, color  = "red", bold = TRUE)
```


```{r}
#| tbl-cap: Performance Metrics by Model

metrics |>
  slice(1:5) |> 
  kbl(col.names = c("", "Week", "Day", "Hour"),
      digits = 2,
      align = c("r", "c", "c", "c"),
     linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |>
  row_spec(5, color  = "red", bold = TRUE)
```

:::

::: {.column width="50%"}


:::
  
::::
  
::: {.notes}
Now, lets return one more time to the performance of these models because I would be remiss if I didn’t complicate the story a bit before we close.  

Clearly, the sensitivity, specificity, and balanced accuracy of all three of these models is encouragingly high

But what is often missed when we evaluate the performance of machine learning models is their positive predictive value or PPV. 
PPV looks at the percentage of positive predictions from the model that are actually true lapses. And PPV is often lower when the positive event, in our case, lapses, is infrequent.

We haven’t talked about the frequency of lapses across the three lapse windows, but it should be intuitively clear that the percent of weeks that contain a lapse will be higher than the percent of days that contain a lapse, and that the percent of hours that contain a lapse will be lower still.  

And this is what we see in our data.

But of course, as we increase the decision threshold for labeling a window as a lapse, we will trade off sensitivity.   We can see this trade off directly in the precision-recall curves on the right.  If we decide we need PPV of at least .75, you can see that we still have reasonable sensitivity for the one week window but we start to miss many lapses in the 1day window and more still in the 1hour window.

I’ll return to this a bit more later when we discuss emerging plans for how best to implement these models within a digital therapeutic.

:::
## Positive Predictive Value (PPV)

:::: {.columns}

::: {.column width="50%"}

```{r}
#| tbl-cap: Lapse Rates by Model 
 
tibble(row_name = "Lapse Rate", Week = "25.4%", Day = "7.7%", Hour = "0.4%") |> 
  kbl(format = "html", col.names = c("", "Week", "Day", "Hour"),
      digits = 2,
      align = c("r", "c", "c", "c"),
     linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |> 
  row_spec(1, color  = "red", bold = TRUE)
```


```{r}
#| tbl-cap: Performance Metrics by Model

metrics |>
  slice(1:5) |> 
  kbl(col.names = c("", "Week", "Day", "Hour"),
      digits = 2,
      align = c("r", "c", "c", "c"),
     linesep = "") |>
  row_spec(row = 0, align = "c") |>
  kable_styling(full_width = FALSE) |>
  kable_classic("striped") |>
  row_spec(5, color  = "red", bold = TRUE)
```

:::

::: {.column width="50%"}

<!--Precision-Recall Curves by Models-->
{{< embed ../../notebooks/ema_figs_pr.qmd#fig-prcurve >}}

:::
  
::::
  
::: {.notes}
Now, lets return one more time to the performance of these models because I would be remiss if I didn’t complicate the story a bit before we close.  

Clearly, the sensitivity, specificity, and balanced accuracy of all three of these models is encouragingly high

But what is often missed when we evaluate the performance of machine learning models is their positive predictive value or PPV. 
PPV looks at the percentage of positive predictions from the model that are actually true lapses. And PPV is often lower when the positive event, in our case, lapses, is infrequent.

We haven’t talked about the frequency of lapses across the three lapse windows, but it should be intuitively clear that the percent of weeks that contain a lapse will be higher than the percent of days that contain a lapse, and that the percent of hours that contain a lapse will be lower still.  

And this is what we see in our data.

But of course, as we increase the decision threshold for labeling a window as a lapse, we will trade off sensitivity.   We can see this trade off directly in the precision-recall curves on the right.  If we decide we need PPV of at least .75, you can see that we still have reasonable sensitivity for the one week window but we start to miss many lapses in the 1day window and more still in the 1hour window.

I’ll return to this a bit more later when we discuss emerging plans for how best to implement these models within a digital therapeutic.

:::

## Key Take Home Messages

- Relatively high combined sensitivity and specificity

- Comparable performance (AUC) from 1 week down to 1 hour windows

- Will need to adjust decision thresholds to fit how we use the algorithm.
  - Lower PPV OK for low burden or low cost recommendations
  - Higher PPV needed to recommend “costly” interventions or actions


::: {.notes}
So to recap our main take-aways from this first project

- We were generally encouraged by these preliminary models.

- We’ve demonstrated that we can get relatively high combined sensitivity and specificity

- We can do this for not only coarse one week windows but also temporally precise windows down to one hour

- But we also recognize that depending on what we will use the predictions for, we may need to adjust decision thresholds to trade off sensitivity for greater positive predictive value.

:::

## (Selective) Next Steps

- Geolocation, cellular communications, and other passively sensed signals


::: {.notes}
One of the key next steps is to develop models that rely more on passive sensing rather than EMA to lower the patient burden of using these systems long term and to gain access to signals that may not be available by self-report.

Let's take a look at two of the more revealing personal sensing methods that we are developing to provide you with some intuition about how we think this will work.

:::

## {#gps_detection_1 data-menu-title="GPS detection, wide view" background-image="https://dionysus.psych.wisc.edu/present/meet_faculty/images/john_gps_wide.png" background-size="100%" background-position="bottom" background-repeat="none"}


::: {.notes}
Here is a wide view of my moment-by-moment location detected by a GPS app over a month when we were first experimenting with this sensing method.  The app recorded the paths that I traveled, with movement by car in green and running in blue.

The red dots indicate places that I stopped to visit for at least a few minutes.

And although not displayed here, the app recorded the days and exact times that I was at each of these locations.

From these data, you can immediately see that I am runner, with long runs leaving from downtown Madison and frequent trail runs on the weekends in the county and state parks to the west and northwest.
:::

## {#gps_detection_2 data-menu-title="GPS detection, zoomed" background-image="https://dionysus.psych.wisc.edu/present/meet_faculty/images/john_gps_zoom.png" background-size="100%" background-position="bottom" background-repeat="none"}


::: {.notes}
Zooming in to the Madison isthmus, these data show that I drove my children halfway around the lake each morning to their elementary school.  And from these data we might be able to detect those stressful mornings when getting my young kids dressed and fed didn't go as planned and we were late, sometimes **very late**, to school!

The app recorded my daily running commute through downtown Madison to and from my office.  From this, we can observe my longs days at the office and also those days that I skipped out.

Looking at the red dots indicating the places I visit, the app can detect the restaurants, bars, and coffee shops where I eat, drink and socialize.  We can use public map data to identify these places and make inferences about what I do there.
:::

## ...Imagine my smartphone communications...{.smaller}

![smartphone_uber.png](https://github.com/jjcurtin/lectures/blob/main/images/smartphone_uber.png?raw=true){.absolute top="15%" left="33%" width="33%" height="auto"}\ 

::: {.notes}
In addition to geolocation, we also collected my smartphone communications logs and even the content of my text messages.

And no such luck, I don't plan to show you my actual text messages!

But imagine what we could learn about me from the patterns of my communications - Who I was calling, when I made those calls, and even the content of what I sent and received by text message.

:::

## Context is Critical


::: {.notes}
We believe we can improve the predictive strength of these geolocation and communication signals even further by identifying the specific people and places that make us happy or sad or stressed, those that we perceive support our mental health and recovery and those who undermine it.

:::

## Context is Critical
![smartphone_context.png](https://github.com/jjcurtin/lectures/blob/main/images/smartphone_context.png?raw=true){.absolute top="15%" left="33%" width="33%" height="auto"}\ 

::: {.notes}


For example, consider the implications of this brief text message thread between a hypothetical patient and their drinking buddy for what you might predict for the probability that they might lapse back to drinking in the coming hours.   

... And how would your prediction change if this wasn’t their drinking buddy but instead, their mom who was a big supporter of their recovery.

This interpersonal context matters!!!

:::

## Context is Critical

<!--intentional blank page-->


::: {.notes}
We can gather this contextual information quickly by asking a few key questions about the people and places we interact with frequently over the first couple of months that we record these signals.  And we can identify these frequent contacts and locations directly from these signals.

In our current projects, we target people and places that we interact with at least twice a month or more for more detailed follow-up to gather context.    And it turns out that this really isn’t that burdensome.   Most of us are creatures of habit and if we set a threshold for 2x monthly interactions, we typically only have 10-30 people and places that meet this threshold.   And it’s the same people and places each month so we can build this context up when the person first starts to use the system and after that it only needs to be updated occasionally when we go somewhere new or make a new friend.

:::

## Contextualized Geolocation

![context_gps.png](https://github.com/jjcurtin/lectures/blob/main/images/context_gps.png?raw=true){width="75%"}\ 

::: {.notes}

:::

## Contextualized Communications

![context_cell.png](https://github.com/jjcurtin/lectures/blob/main/images/context_cell.png?raw=true){width="75%"}\ 

::: {.notes}

:::

## Baseline Feature Engineering for GPS

:::: {.columns}
::: {.column width="50%"}

- Focus on recent past experiences (6, 12, 24, 48, 72, 168 hours)

- Raw scores and change scores (from baseline)

- Time spent at important places (e.g, alcohol present, drank at location in past, risky, unpleasant)

:::
::: {.column width="50%"}
![feature_gps.png](https://github.com/jjcurtin/lectures/blob/main/images/feature_gps.png?raw=true)\ 
:::
::::


::: {.notes}
To be clear, we have only just begun to train models using GPS.   We have started by focusing on recent locations…..
DESCRIBE MORE


There is clearly predictive signal in the geolocation signal but not enough to stand alone as the only features at this point.   We are seeing AUCs in the low .7s when we use only geolocation to predict future lapses.  

However, when we add GPS to the EMA models that I described today, we appear to need fewer unique EMA items to get the same level of model performance.   So the addition of GPS may serve to lower patient burden while maintaining model performance.

[PAUSE]

The story is pretty much the same right now for cellular communications as well.   At this point we have only worked with the meta data – the contact numbers, call times, durations, etc – combined with context.   We haven’t yet done anything with the text message content.   And models built from the meta data perform about as well at these preliminary GPS models.

But we are hoping to extract more signal from the both of these as we continue to work on feature engineering with it.

:::

## (Selective) Next Steps

- [Geolocation, cellular communications, and other passively sensed signals]{.gray}

- Build models with lead times > 0 hours





::: {.notes}
NEED TEXT FOR THIS BUT ITS IMPORTANT.   FOCUS ON lead = 0 for 1 hour and JIT – distraction, urge surfing but lead = 1 week for contact with therapist, sponsor, supportive friends and family, etc.

:::


## (Selective) Next Steps


- [Geolocation, cellular communications, and other passively sensed signals]{.gray}

- [Build models with lead times > 0 hours]{.gray}

- More diversity in training data

::: {.notes}
We are excited by the early performance of these machine learning models BUT I want to be clear that these are preliminary research studies.  And they included mostly white participants from our local community in Madison, WI.  

Models trained on these participants would be unlikely to work well with black and brown patients or patients from rural communities.  

Machine learning models must be trained on diverse samples of patients or their use may exacerbate rather than reduce existing mental healthcare disparities. 
:::

## Active Project: Lapse in patients with Opioid Use Disorder 

::: {.medium}
- Recruiting 400 patients in recovery from Opioid Use Disorder (All enrolled, many completed )
- National sample (size; diversity: demographics, location)
- More variation in stage of recover (1 – 6 months at start)
- 12 months of monitoring
- Closer to real implementation methods
:::
:::: {.columns}

::: {.column width="60%"}
![risk2_pis.png](https://github.com/jjcurtin/lectures/blob/main/images/risk2_pis.png?raw=true)\ 
:::
::: {.column width="40%"}
![nida_logo.png](https://github.com/jjcurtin/lectures/blob/main/images/nida_logo.png?raw=true)\ 
:::
::::
::: {.notes}
We are now collecting data for a NIDA funded project where we are specifically recruiting for racial, ethnic, and geographic diversity across the entire United States.


We are also recruiting for people at different stages in their recovery and following them for a longer period of time – up to 12 months

:::

## (Selective) Next Steps


- [Geolocation, cellular communications, and other passively sensed signals]{.gray}

- [Build models with lead times > 0 hours]{.gray}

- [More diversity in training data]{.gray}

- Use models to improve DTx engagement and clinical outcomes
  - SMART DTx – algorithm guided use
  - How to craft patient feedback to encourage trust in the algorithm


::: {.notes}
Of course, accurately predicting future lapses is only useful if these predictions can be used to sustain engagement with treatments and improve clinical outcomes.  

We have now started to consider how we can use these predictions to help patients optimize their use of a digital therapeutic.  

These models may be able to suggest when more use of the DTx is needed because lapse risk is increasing or high but we are also hoping that we can use the models to recommend which tools and supports in the DTx are best for that patient at that moment in time given their lapse risk probability and important features contributing to the lapse prediction.

We are also sensitive to the fact that these recommendations from the machine learning models will need to be provided to patients in a transparent manner that also encourages them to trust the algorithm and follow its recommendations.  

:::

## Relapse Prevention Model

![relapse_prevention_flowchart.png](https://github.com/jjcurtin/lectures/blob/main/images/relapse_prevention_flowchart.png?raw=true)\ 


::: {.notes}

:::

## Optimization of an Algorithm Guided Smart DTx {.medium}

- Lapse probabilities updated daily based on EMA and Geolocation features
- Use lapse probability and locally important features to [recommend optimal DTx modules]{.red} – guided by Relapse Prevention model
- Provide recommendations with factorial manipulation of  [algorithmic transparency]{.red}
  - risk level 
  - risk change
  - key important features
- Outcomes
  - Engagement with recommended module
  - Heavy drinking days
  
![nida_logo.png](https://github.com/jjcurtin/lectures/blob/main/images/niaaa_logo.png?raw=true){.absolute bottom="5%" right="2%" width="auto" height="auto"}\ 

::: {.notes}

:::

## Data Science Needs

- Pipeline for Daily Module Recommendations
  - Pull sensed data from cloud
  - Feature engineer sensed data
  - Generate model lapse probability prediction
  - Generate local SHAPs
  - Map all of this to recommendation
  - Combine with transparancy features
  - Push up to DTx
  
- Data validity checking and EDA for OUD project

- Develop Sensing App (Apple ResearchKit?  Android?)

- Lots of Modeling
  - lag model (Kendra)
  - gps model (Claire)
  - communications model (Coco)
  - AUD screening using social media (Sarah) 

## CRediTs

![credits.png](https://github.com/jjcurtin/lectures/blob/main/images/credits_risk1.png?raw=true)\ 

::: {.notes}

:::


## Demographics & Alcohol Use History
<!-- JOHN looks like the latest version of the paper combined these tables - is that okay? --> 

<!-- Demographics table-->
```{r}
# footnote_table_dem_a <- "N = 151"
# footnote_table_dem_b <- "Two participants reported 100 or more quit attempts. We removed these outliers prior to calculating the mean (M), standard deviation (SD), and range."
```

```{r table_demo_auh}
# options(knitr.kable.NA = "")
# 
# dem <- screen %>% 
#   summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
#             SD = as.character(round(sd(dem_1, na.rm = TRUE), 1)),
#             min = as.character(min(dem_1, na.rm = TRUE)),
#             max = as.character(max(dem_1, na.rm = TRUE))) %>% 
#   mutate(var = "Age",
#          n = as.numeric(""),
#          perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()) %>% 
#   full_join(screen %>% 
#   select(var = dem_2) %>% 
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = dem_3) %>% 
#   mutate(var = fct_relevel(factor(var, 
#                          c("American Indian/Alaska Native", "Asian", "Black/African American",
#                            "White/Caucasian", "Other/Multiracial")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = dem_4) %>% 
#   mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
#                          TRUE ~ "Yes"),
#          var = fct_relevel(factor(var, c("Yes", "No")))) %>% 
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = dem_5) %>% 
#   mutate(var = fct_relevel(factor(var, 
#                          c("Less than high school or GED degree", "High school or GED", 
#                            "Some college", "2-Year degree", "College degree", "Advanced degree")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = dem_6, dem_6_1) %>% 
#   mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
#                          dem_6_1 == "Part-time" ~ "Employed part-time",
#                          TRUE ~ var)) %>% 
#   mutate(var = fct_relevel(factor(var, 
#                          c("Employed full-time", "Employed part-time", "Full-time student",
#                            "Homemaker", "Disabled", "Retired", "Unemployed", 
#                            "Temporarily laid off, sick leave, or maternity leave",
#                            "Other, not otherwise specified")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   summarise(mean = format(round(mean(dem_7, na.rm = TRUE), 0), big.mark = ","),
#             SD = format(round(sd(dem_7, na.rm = TRUE), 0), big.mark = ","),
#             min =format(round(min(dem_7, na.rm = TRUE), 0), big.mark = ","),
#             max = format(round(max(dem_7, na.rm = TRUE), 0), scientific = FALSE, big.mark = ",")) %>% 
#   mutate(var = "Personal Income",
#         n = as.numeric(""),
#         perc = as.numeric(""),
#         mean = str_c("$", as.character(mean)),
#         SD = str_c("$", as.character(SD)),
#         min = str_c("$", as.character(min)),
#         max = as.character(max)) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", "min", "max")) %>% 
#   full_join(screen %>% 
#   select(var = dem_8) %>% 
#   mutate(var = case_when(var == "Never Married" ~ "Never married",
#                          TRUE ~ var)) %>% 
#   mutate(var = fct_relevel(factor(var, 
#                          c("Never married", "Married", "Divorced", "Separated",
#                            "Widowed")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))
# 
# auh <- screen %>% 
#   summarise(mean = mean(auh_1, na.rm = TRUE),
#             SD = sd(auh_1, na.rm = TRUE),
#             min = min(auh_1, na.rm = TRUE),
#             max = max(auh_1, na.rm = TRUE)) %>% 
#   mutate(var = "Age of first drink",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()) %>% 
#   full_join(screen %>% 
#   summarise(mean = mean(auh_2, na.rm = TRUE),
#             SD = sd(auh_2, na.rm = TRUE),
#             min = min(auh_2, na.rm = TRUE),
#             max = max(auh_2, na.rm = TRUE)) %>% 
#   mutate(var = "Age of regular drinking",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", 
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   summarise(mean = mean(auh_3, na.rm = TRUE),
#             SD = sd(auh_3, na.rm = TRUE),
#             min = min(auh_3, na.rm = TRUE),
#             max = max(auh_3, na.rm = TRUE)) %>% 
#   mutate(var = "Age at which drinking became problematic",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   summarise(mean = mean(auh_4, na.rm = TRUE),
#             SD = sd(auh_4, na.rm = TRUE),
#             min = min(auh_4, na.rm = TRUE),
#             max = max(auh_4, na.rm = TRUE)) %>% 
#   mutate(var = "Age of first quit attempt",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table
#   filter(auh_5 < 100) %>% 
#   summarise(mean = mean(auh_5, na.rm = TRUE),
#             SD = sd(auh_5, na.rm = TRUE),
#             min = min(auh_5, na.rm = TRUE),
#             max = max(auh_5, na.rm = TRUE)) %>% 
#   mutate(var = "Number of Quit Attempts*",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_1) %>%
#   mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ months)",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_2) %>%
#   mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 months)",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_3) %>%
#   mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_4) %>%
#   mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_5) %>%
#   mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
#                          TRUE ~ var)) %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_6) %>%
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_6_7) %>%
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = auh_7) %>% 
#   mutate(var = fct_relevel(factor(var, c("Yes", "No")))) %>%
#   group_by(var) %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) %>% 
#   rowwise() %>% 
#   # calculate dsm5 score by adding up dsm5_1 through dsm5_11
#   mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, 
#                             dsm5_8, dsm5_9, dsm5_10, dsm5_11))) %>% 
#   ungroup() %>% 
#   summarise(mean = mean(dsm5_total),
#             SD = sd(dsm5_total),
#             min = min(dsm5_total, na.rm = TRUE),
#             max = max(dsm5_total, na.rm = TRUE)) %>% 
#   mutate(var = "Alcohol Use Disorder DSM-5 Symptom Count",
#         n = as.numeric(""),
#         perc = as.numeric("")) %>% 
#   select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
#                                              "min", "max")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_1) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_2) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Cannabis (marijuana, pot, grass, hash, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_3) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Cocaine (coke, crack, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_4) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_5) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Inhalants (nitrous, glue, petrol, paint thinner, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_6) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_7) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
#   full_join(screen %>% 
#   select(var = assist_2_8) %>%
#   filter(var != "Never" & !is.na(var)) %>% 
#   mutate(var = "Opioids (heroin, morphine, methadone, codeine, etc.)") %>% 
#   group_by(var) %>% 
#   drop_na() %>% 
#   summarise(n = n()) %>% 
#   mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) 
```


```{r}
# display and format table
# dem %>% 
#   bind_rows(auh %>% 
#               mutate(across(mean:max, ~round(.x, 1))) %>% 
#               mutate(across(mean:max, ~as.character(.x)))) %>% 
#   mutate(range = str_c(min, "-", max)) %>%
#   select(-c(min, max)) %>% 
#   kbl(longtable = TRUE,
#       booktabs = TRUE,
#       col.names = c("", "N", "%", "M", "SD", "Range"),
#       align = c("l", "c", "c", "c", "c", "c"),
#       digits = 1,
#       caption = "Demographics and Alcohol Use Information") %>%
#   kable_styling(font_size = 20, full_width = FALSE) %>% 
#   kable_classic("striped") |>
#   row_spec(row = 0, align = "c", italic = TRUE) %>% 
#   pack_rows("Sex", 2, 3, bold = FALSE) %>% 
#   pack_rows("Race", 4, 8, bold = FALSE) %>% 
#   pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) %>% 
#   pack_rows("Education", 11, 16, bold = FALSE) %>% 
#   pack_rows("Employment", 17, 25, bold = FALSE) %>% 
#   pack_rows("Marital Status", 27, 31, bold = FALSE) %>% 
#   pack_rows("Alcohol Use Disorder Milestones", 32, 35, bold = FALSE) %>% 
#   pack_rows("Lifetime History of Treatment (Can choose more than 1)", 37, 43, bold = FALSE) %>% 
#   pack_rows("Received Medication for Alcohol Use Disorder", 44, 45, bold = FALSE) %>% 
#   pack_rows("Current (Past 3 Month) Drug Use", 47, 54, bold = FALSE) %>% 
#   footnote(general=footnote_table_dem_a, symbol = c(footnote_table_dem_b), 
#            threeparttable = TRUE)
```


## Consort Diagram

```{r load_consort_data}
# disposition <- read_csv(file.path(path_data_ema, "disposition.csv"), col_types = "ccDDcccccccccc")
```


```{r load_model_data}
# AUCs for 10x10 folds data
# auc_folds_week<- readRDS(file.path(path_models, "resample_metrics_best_all_1week_0_v4_kfold.rds"))
# auc_folds_day<- readRDS(file.path(path_models, "resample_metrics_best_all_1day_0_v4_kfold.rds"))
# auc_folds_hour<- readRDS(file.path(path_models, "resample_metrics_best_all_1hour_0_v4_kfold.rds")) 
# 
# # posterior probabilites
# pp <- readRDS(file.path(path_models, "posteriors_all_allwindows_0_v4_kfold.rds"))
# 

# # Predictions data
# preds_week<- readRDS(file.path(path_models, "resample_preds_best_all_1week_0_v4_kfold.rds"))
# preds_day<- readRDS(file.path(path_models, "resample_preds_best_all_1day_0_v4_kfold.rds"))
# preds_hour<- readRDS(file.path(path_models, "resample_preds_best_all_1hour_0_v4_kfold.rds")) 
# 
# # roc overall
# roc_week_full <- preds_week %>% 
#   roc_curve(prob, truth = truth)
# 
# roc_day_full <- preds_day %>% 
#   roc_curve(prob, truth = truth)
# 
# roc_hour_full <- preds_hour%>% 
#   roc_curve(prob, truth = truth)
# 
# # rocs per fold
# roc_week <- preds_week %>%
#   nest(.by = split_num, .key = "preds") %>% 
#   mutate(roc = map(preds, \(preds) roc_curve(preds, prob, 
#                                              truth = truth))) %>% 
#   mutate(model = "week")
# 
# roc_day <- preds_day %>%
#   nest(.by = split_num, .key = "preds") %>% 
#   mutate(roc = map(preds, \(preds) roc_curve(preds, prob, 
#                                              truth = truth))) %>% 
#   mutate(model = "day")
# 
# roc_hour <- preds_hour %>%
#   nest(.by = split_num, .key = "preds") %>% 
#   mutate(roc = map(preds, \(preds) roc_curve(preds, prob, 
#                                              truth = truth))) %>% 
#   mutate(model = "week")
# 
# # pr overall
# pr_week_full <- preds_week %>% 
#   pr_curve(prob, truth = truth)
# 
# pr_day_full <- preds_day %>% 
#   pr_curve(prob, truth = truth)
# 
# pr_hour_full <- preds_hour%>% 
#   pr_curve(prob, truth = truth)
# 
# # prs per fold
# pr_week <- preds_week %>% 
#   pr_curve(prob, truth = truth) %>% 
#   mutate(model = "1week")
# 
# pr_day <- preds_day %>% 
#   pr_curve(prob, truth = truth) %>% 
#   mutate(model = "1day")
# 
# pr_hour <- preds_hour%>% 
#   pr_curve(prob, truth = truth) %>% 
#   mutate(model = "1hour")
# 
# pr_all <- pr_week %>% 
#   bind_rows(pr_day) %>% 
#   bind_rows(pr_hour)
# 
# #raw SHAPs
# shap_raw_week <- readRDS(file.path(path_models, "imp_shap_raw_all_1week_0_v4.rds")) %>% 
#   group_by(variable) %>% 
#   slice(1) %>%   
#   ungroup() %>% 
#   arrange(mean_value)
# shap_raw_day <- readRDS(file.path(path_models, "imp_shap_raw_all_1day_0_v4.rds")) %>% 
#   group_by(variable) %>% 
#   slice(1) %>%   
#   ungroup() %>% 
#   arrange(mean_value)
# shap_raw_hour <- readRDS(file.path(path_models, "imp_shap_raw_all_1hour_0_v4.rds")) %>% 
#   group_by(variable) %>% 
#   slice(1) %>%   
#   ungroup() %>% 
#   arrange(mean_value)
# 
# # Grouped SHAPS
# shap_grouped_week <- readRDS(file.path(path_models, "imp_shap_grouped_all_1week_0_v4.rds")) 
# shap_grouped_day <- readRDS(file.path(path_models, "imp_shap_grouped_all_1day_0_v4.rds"))
# shap_grouped_hour <- readRDS(file.path(path_models, "imp_shap_grouped_all_1hour_0_v4.rds")) 
# 
# # lapse labels
# labels_week <- read_csv(file.path(path_data_ema, "labels_1week.csv"), col_types = cols())
# labels_day <- read_csv(file.path(path_data_ema, "labels_1day.csv"), col_types = cols())
# labels_hour <- read_csv(file.path(path_data_ema, "labels_1hour.csv"), col_types = cols())
```

<!-- Consort Diagram-->

```{r caption_consort}
fig_caption_consort <- "Consort Diagram"
```



```{r fig_consort}
#| fig.cap: !expr fig_caption_consort
#| fig.height: 7
# consort_plot(data = disposition,
#              orders = c(eligible = "Eligible Sample",
#                         consented_reason = "Not Consented",
#                         consented = "Consented",
#                         enrolled_reason = "Not Enrolled",
#                         enrolled = "Enrolled",
#                         completed_followup_reason = "Discontinued",
#                         completed_followup = "Completed through Followup 1",
#                         analysis_reason = "Excluded",
#                         analysis = "Final Analysis"),
#              side_box = c("consented_reason", 
#                           "enrolled_reason", 
#                           "completed_followup_reason",
#                           "analysis_reason"),
#              cex = .9,
#              text_width = 45)
```

## ROC Posterior Probabilities

<!-- AUC figure by model w/posterior probabilities-->  

```{r fig_roc_pp}
#| fig.height: 4.5  
#| fig.width: 7  
#| fig-align: "center"


# roc_plot <- roc_all %>% 
#   mutate(model = factor(model, levels = c("1week", "1day", "1hour"), 
#                         labels = c("Week", "Day", "Hour"))) %>% 
#   ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
#   geom_path(show.legend = FALSE) +
#   geom_abline(lty = 3) +
#   coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
#   labs(x = "Specificity",
#        y = "Sensitivity") +
#   scale_x_continuous(breaks = seq(0,1,.25),
#     labels = sprintf("%.2f", seq(1,0,-.25))) 
# 
# pp_tidy <- pp %>% 
#   tidy(seed = 123)
# 
# ci <- pp_tidy %>% 
#   summary() %>% 
#   mutate(model = factor(model, levels = c("week", "day", "hour"),
#                         labels = c("Week", "Day", "Hour")),
#          y = 1000)
# 
# pp_plot <- pp_tidy %>% 
#   mutate(model = factor(model, levels = c("week", "day", "hour"),
#                         labels = c("Week", "Day", "Hour"))) %>%
#   ggplot() + 
#   geom_histogram(aes(x = posterior, fill = model), color = "black", alpha = .4, 
#                  bins = 30) +
#   geom_segment(mapping = aes(y = y+100, yend = y-100, x = mean, xend = mean,
#                            color = model),
#                show.legend = FALSE,
#                data = ci) +
#   geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),
#               show.legend = FALSE,
#                data = ci) +
#   geom_text(data = ci, x = c(.93, .907, .92), y = 1000, 
#             label = str_c(round(ci$mean, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
#   facet_wrap(~model, ncol = 1) +
#   scale_y_continuous("Posterior Probability", breaks = c(0, 500, 1000)) +
#   xlab("Area Under ROC Curve") +
#   theme(strip.background = element_blank(),
#         strip.text.x = element_blank())
# 
# roc_plot + pp_plot +
#   plot_layout (guides = "collect") &
#   theme(legend.position = "bottom")
```

## Model Comparison Posterior Probabilites



```{r caption_pp_contrasts}
fig_caption_pp_contrasts <- "Posterior Probabilities for Model Contrasts for AUC.   Region of Practical Equivalence (ROPE) indicated by dashed yellow lines"
```

```{r fig_posterior_d}
#| fig-cap: !expr fig_caption_pp_contrasts
#| fig.height:  7

# ci <- pp %>% 
#   contrast_models(list("hour","hour", "day"), 
#                 list("week", "day", "week")) %>% 
#   summary(size = .01) %>% 
#   mutate(contrast = factor(contrast, 
#                            levels = c("hour vs week", "hour vs day", "day vs week"),
#                            labels = c("Hour vs. Week", "Hour vs. Day", "Day vs. Week")),
#          y = 700)
# 
# pp %>% 
#   tidy(seed = 123) %>%   
#   group_by(model) %>% 
#   mutate(sample = row_number()) %>% 
#   ungroup() %>% 
#   pivot_wider(names_from = model, values_from = posterior) %>% 
#   mutate(hour_vs_week = hour - week,
#          hour_vs_day = hour - day,
#          day_vs_week = day - week) %>% 
#   pivot_longer(cols = hour_vs_week:day_vs_week,
#                names_to = "contrast",
#                values_to = "posterior") %>% 
#   mutate(contrast = factor(contrast, 
#                            levels = c("hour_vs_week", "hour_vs_day", "day_vs_week"),
#                            labels = c("Hour vs. Week", "Hour vs. Day", "Day vs. Week"))) %>% 
#   ggplot() +
#   geom_histogram(aes(x = posterior, fill = contrast), 
#                  color = "black", alpha = .4, bins = 30) +
#   geom_vline(xintercept = -.01, color = "yellow", linetype = "dashed", size = 1) +
#   geom_vline(xintercept = .01, color = "yellow", linetype = "dashed", size = 1) +
#   geom_segment(mapping = aes(y = y+100, yend = y-100, x = mean, xend = mean,
#                              color = contrast), data = ci, show.legend = FALSE) +
#   geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, 
#                              color = contrast), data = ci, show.legend = FALSE) +
#   geom_text(data = ci, x = c(.0255, .035, .015), y = 700, 
#             label = str_c(round(ci$mean, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
#   facet_wrap(~contrast, ncol = 1) +
#   xlab("Posterior")
  
```
